
Loading Packages
```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(forecast)
library(lubridate)
library(corrplot)
library(car)
library(corrr)
```

Preparing Dataset
```{r message=FALSE, warning=FALSE}
df <- read_csv('final_df_6m.csv')
df$date <- as.Date(df$date)
summary(df$y_lr_6m)
```

## 1. Time Plot Analysis
```{r}
time_plot_6m <- ggplot(df, aes(x = date, y = y_lr_6m)) +
  geom_line(color = "steelblue") +
  labs(
    title = "Time Plot of Log Oil Price Returns (6m)",
    subtitle = "Checking for stationarity, homoscedasticity, and structural breaks",
    x = "Date", y = "Log Return") +
  theme_minimal()
print(time_plot_6m)
```

The time plot of y_lr_6m, the target variable shows several crucial characteristics. First, the series is mean-reverting and stationary. Secondly, it displays distinct periods of volatility clustering and significant structural breaks corresponding to major economic events (eg 2008 and 2020). These characteristics violate the homoscedasticity assumption of linear models, and hence favor machine learning methods that can handle non-constant variance and high-magnitude outliers. Thirdly, the presence of these breaks mandates the use of rolling-window or walk-forward validation over a fixed train-test split to ensure that the final model's reliability is accurately assessed under changing market trends.

## 2. Density Plot Analysis
```{r}
# Density Plot
density_plot_6m <- ggplot(df, aes(x = y_lr_6m)) +
  geom_density(color = "red", linewidth = 1) +
  labs(
    title = "Distribution of Log Oil Price Returns (6m)",
    x = "Log Return", y = "Density") +
  theme_minimal()
print(density_plot_6m)
```

The density plot of the 6-month log returns shows the data is not normally distributed, maintaining the negative skew (heavier left tail) observed in the 3-month plot, with returns extending significantly further on the negative side (exceeding -1.0) than on the positive side. This suggests that over a 6-month horizon, large price drops (negative returns) are a more prominent feature than large price rallies. This shows that extreme market movements—big crashes happen frequently and cannot be predicted by a normal distribution. This asymmetry highlights that the risk is unbalanced, which warrants the use of risk-adjusted performance metrics that penalise downside risk more heavily.

## 3. ACF & PACF Analysis
Autocorrelation (ACF) and Partial Autocorrelation (PACF) plots are used to analyse the "memory" of the time series—how past values of the returns are correlated with the present value.
```{r}
target_ts <- ts(df$y_lr_6m, frequency = 12)

par(mfrow = c(2, 1))
forecast::Acf(target_ts, main = "Autocorrelation Function (ACF) for Oil Returns", na.action = na.pass)
forecast::Pacf(target_ts, main = "Partial Autocorrelation Function (PACF) for Oil Returns", na.action = na.pass)
par(mfrow = c(1, 1)) 
```
The ACF measures the total correlation between the current oil return and its past values. There is a statistically significant positive correlation at lag 1 to 3. After that, the correlations quickly drop into the insignificant range, though smaller spikes appear in later lags.

The PACF measures the direct correlation between the current oil return and a past value after removing the influence of all the shorter, intervening lags. This helps isolate the unique effect of each specific lag. There is a very strong, statistically significant spike at lag 1 and 2. After lag 2, the correlations immediately "cut off" and become statistically insignificant, though smaller spikes appear in later lags. This pattern reflects that the oil return from one and two month prior is a direct and important predictor of the current month's return. (AR(1), AR(2))

## 4. Cross-Correlation (CCF) Analysis
This analysis searches for leading indicators (-ve lags) by calculating the cross-correlation between the current value of each predictor and the future values of the target variable, y_lr_6m.
```{r}
df2 <- df %>%
  select(-date, -y_lr_6m) %>% select(ends_with("_l0")) %>%
  names()


ccf_list <- list()
for (predictor in df2) {

  if (is.numeric(df[[predictor]]) && sd(df[[predictor]], na.rm = TRUE) > 0) {
    ccf_object <- forecast::Ccf(
      x = df[[predictor]],
      y = df$y_lr_6m,
      na.action = na.pass,
      plot = FALSE
    )
    
    correlations <- as.vector(ccf_object$acf)
    lags <- as.vector(ccf_object$lag)
    leading_lags <- lags < 0
    
    if (any(leading_lags)) {
      leading_correlations <- correlations[leading_lags]
      leading_lags <- lags[leading_lags]
      max_abs_cor_index <- which.max(abs(leading_correlations))
      
      max_correlation <- leading_correlations[max_abs_cor_index]
      lag_at_max <- leading_lags[max_abs_cor_index]

      ccf_list[[predictor]] <- tibble(
        Predictor = predictor,
        LeadingCorrelation = max_correlation,
        BestLeadingLag = lag_at_max
      )
    }
  }
}

ccf <- bind_rows(ccf_list)%>%
  arrange(desc(abs(LeadingCorrelation)))

print(ccf, n = 20)
```

## 5. Multicollinearity (same across different time horizons)
This analysis identifies multicollinearity through first identifying the most highly correlated pairs and second visualising the relationships in a heatmap.
```{r}
# find only numeric predictors
df1 <- df %>%
  select_if(is.numeric) %>%
  select(-y_lr_6m) %>%
  select(ends_with('_l0'))

# full correlated matrix
full_cor_matrix <- corrr::correlate(df1)

#finding pairs with high correlation, ignoring intra-variable lags
highly_correlated_pairs <- full_cor_matrix %>%
  shave() %>%
  corrr::stretch(na.rm = TRUE) %>%
  filter(abs(r) > 0.9) %>%
  arrange(desc(abs(r)))

print(distinct(highly_correlated_pairs, x, y,.keep_all = TRUE), n = 30)


cor_matrix_lag0 <- cor(df1 %>%
  select(ends_with("_l0")), use = "complete.obs")

corrplot(cor_matrix_lag0,
           method = "color",         
           type = "upper",           
           order = "hclust",         
           tl.col = "black",         
           tl.cex = 0.5,             
           #tl.pos = 'n', #to not show texts
           diag = FALSE,
           title = "\n\nCorrelation Heatmap of Lag 0 Predictors",
           mar = c(0, 0, 1, 0))
```
The Correlation Heatmap of Lag 0 Predictors shows multicollinearity across our feature set. Visually, the large clusters of dark red and blue squares show that many features are highly correlated with one another, often above $|r| > 0.9$.

## 6. Principal Component Analysis (PCA) (same across different time horizons)
```{r}
df3 <- df %>%
  select_if(is.numeric) %>%
  select(-y_lr_6m)

if (nrow(df3) > 1 && ncol(df3) > 1) {
  pca_result <- prcomp(df3, center = TRUE, scale. = TRUE)
  
  screeplot(pca_result, type = "lines", main = "Scree Plot for PCA")}
  print(summary(pca_result)$importance[, 1:min(15, ncol(df1))])
```


Loading Packages
```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(forecast)
library(lubridate)
library(corrplot)
library(car)
library(corrr)
```

Preparing Dataset
```{r message=FALSE, warning=FALSE}
df <- read_csv('final_df_12m.csv')
df$date <- as.Date(df$date)
summary(df$y_lr_12m)
```

## 1. Time Plot Analysis
```{r}
time_plot_12m <- ggplot(df, aes(x = date, y = y_lr_12m)) +
  geom_line(color = "steelblue") +
  labs(
    title = "Time Plot of Log Oil Price Returns (12m)",
    subtitle = "Checking for stationarity, homoscedasticity, and structural breaks",
    x = "Date", y = "Log Return") +
  theme_minimal()
print(time_plot_12m)
```

The time plot of y_lr_12m, the target variable shows several crucial characteristics. First, the series is mean-reverting and stationary. Secondly, it displays distinct periods of volatility clustering and significant structural breaks corresponding to major economic events (eg 2008 and 2020). These characteristics violate the homoscedasticity assumption of linear models, and hence favor machine learning methods that can handle non-constant variance and high-magnitude outliers. Thirdly, the presence of these breaks mandates the use of rolling-window or walk-forward validation over a fixed train-test split to ensure that the final model's reliability is accurately assessed under changing market trends.

## 2. Density Plot Analysis
```{r}
# Density Plot
density_plot_12m <- ggplot(df, aes(x = y_lr_12m)) +
  geom_density(color = "red", linewidth = 1) +
  labs(
    title = "Distribution of Log Oil Price Returns (12m)",
    x = "Log Return", y = "Density") +
  theme_minimal()
print(density_plot_12m)
```

The density plot of the 12-month log returns shows the data is not normally distributed, where there are long tails on both the left and right sides of the plot. The curve extends far out to -1.0 and beyond 1.0, representing very large negative and positive returns, showing that extreme market movements—big crashes happen frequently and cannot be predicted by a normal distribution. This is especially crucial for LASSO and Ridge regression models that rely on the assumption of normally distributed residuals. The plot also shows some asymmetry (skewness), and more weight on the left tail, suggesting downside risk might be marginally heavier than upside risk. Therefore, the model evaluation should not rely solely on standard error metrics like RMSE but should also include risk-adjusted performance metrics that penalise downside risk more heavily.

## 3. ACF & PACF Analysis
Autocorrelation (ACF) and Partial Autocorrelation (PACF) plots are used to analyse the "memory" of the time series—how past values of the returns are correlated with the present value.
```{r}
target_ts <- ts(df$y_lr_12m, frequency = 12)

par(mfrow = c(2, 1))
forecast::Acf(target_ts, main = "Autocorrelation Function (ACF) for Oil Returns", na.action = na.pass)
forecast::Pacf(target_ts, main = "Partial Autocorrelation Function (PACF) for Oil Returns", na.action = na.pass)
par(mfrow = c(1, 1)) 
```
The ACF measures the total correlation between the current oil return and its past values. There is a statistically significant positive correlation at lag 1 to 7. After that, the correlations quickly drop into the insignificant range and appear to tail off.
The PACF measures the direct correlation between the current oil return and a past value after removing the influence of all the shorter, intervening lags. This helps isolate the unique effect of each specific lag. There is a very strong, statistically significant spike at lag 1. After lag 1, the correlations immediately "cut off" and become statistically insignificant until a smaller spike appears at lag 13. This pattern reflects that the oil return from one month prior is a direct and important predictor of the current month's return. (AR(1))

## 4. Cross-Correlation (CCF) Analysis
This analysis searches for leading indicators (-ve lags) by calculating the cross-correlation between the current value of each predictor and the future values of the target variable, y_lr_12m.
```{r}
df2 <- df %>%
  select(-date, -y_lr_12m) %>% select(ends_with("_l0")) %>% 
  names()
#dont include other lags just to see the rls between different variables (same base variables but different lags tend to have very high correlation)

ccf_list <- list()
for (predictor in df2) {

  if (is.numeric(df[[predictor]]) && sd(df[[predictor]], na.rm = TRUE) > 0) {
    ccf_object <- forecast::Ccf(
      x = df[[predictor]],
      y = df$y_lr_12m,
      na.action = na.pass,
      plot = FALSE
    )
    
    correlations <- as.vector(ccf_object$acf)
    lags <- as.vector(ccf_object$lag)
    leading_lags <- lags < 0
    
    if (any(leading_lags)) {
      leading_correlations <- correlations[leading_lags]
      leading_lags <- lags[leading_lags]
      max_abs_cor_index <- which.max(abs(leading_correlations))
      
      max_correlation <- leading_correlations[max_abs_cor_index]
      lag_at_max <- leading_lags[max_abs_cor_index]

      ccf_list[[predictor]] <- tibble(
        Predictor = predictor,
        LeadingCorrelation = max_correlation,
        BestLeadingLag = lag_at_max
      )
    }
  }
}

ccf <- bind_rows(ccf_list)%>%
  arrange(desc(abs(LeadingCorrelation)))

print(ccf, n = 20)
```

## 5. Multicollinearity (same across different time horizons)
This analysis identifies multicollinearity through first identifying the most highly correlated pairs and second visualising the relationships in a heatmap.
```{r}
# find only numeric predictors
df1 <- df %>%
  select_if(is.numeric) %>%
  select(-y_lr_12m) %>%
  select(ends_with('_l0'))

# full correlated matrix
full_cor_matrix <- corrr::correlate(df1)

#finding pairs with high correlation, ignoring intra-variable lags
highly_correlated_pairs <- full_cor_matrix %>%
  shave() %>%
  corrr::stretch(na.rm = TRUE) %>%
  filter(abs(r) > 0.9) %>%
  arrange(desc(abs(r)))

print(distinct(highly_correlated_pairs, x, y,.keep_all = TRUE), n = 30)


cor_matrix_lag0 <- cor(df1 %>%
  select(ends_with("_l0")), use = "complete.obs")

corrplot(cor_matrix_lag0,
           method = "color",         
           type = "upper",           
           order = "hclust",         
           tl.col = "black",         
           tl.cex = 0.5,             
           #tl.pos = 'n', #to not show texts
           diag = FALSE,
           title = "\n\nCorrelation Heatmap of Lag 0 Predictors",
           mar = c(0, 0, 1, 0))
```
The Correlation Heatmap of Lag 0 Predictors shows multicollinearity across our feature set. Visually, the large clusters of dark red and blue squares show that many features are highly correlated with one another, often above $|r| > 0.9$.

## 6. Principal Component Analysis (PCA) (same across different time horizons)
```{r}
df3 <- df %>%
  select_if(is.numeric) %>%
  select(-y_lr_12m)

if (nrow(df3) > 1 && ncol(df3) > 1) {
  pca_result <- prcomp(df3, center = TRUE, scale. = TRUE)
  
  screeplot(pca_result, type = "lines", main = "Scree Plot for PCA")}
  print(summary(pca_result)$importance[, 1:min(15, ncol(df1))])
```

---
title: "Data Cleaning EC4308"
author: "Tay Le Rui"
date: "2025-10-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(slider)      # rolling features without leakage
library(zoo)         # na.locf
library(recipes)     # scaling on train only
library(data.table)  # fast ops
library(BVAR)

```

## R Markdown

Importing data

```{r}
raw_data <-read.csv("merged_df_final.csv",check.names = FALSE)
raw_data
```




Checking for null values
```{r}
colSums(is.na(raw_data))[colSums(is.na(raw_data)) > 0]

```
```{r}
# Just to check how the null row looks like relative to the other rows
raw_data %>%
  select(date, `3_month_aa_financial_commercial_paper_rate`, `3_month_commercial_paper_minus_fedfunds`) %>%
  filter( (date > '2020-01-01') & (date <= '2020-05-01'))
  
```

```{r}
# For simplicity lets use ffill
clean_data <- raw_data %>%
  arrange(date) %>%  # ensure chronological order
  fill(
    `3_month_aa_financial_commercial_paper_rate`,
    `3_month_commercial_paper_minus_fedfunds`,
    .direction = "down"
  )
```

```{r}
#Last check
sum(colSums(is.na(clean_data)))
```


Perfect there is no missing values

```{r}
#Getting the features
num_cols = clean_data %>% select(where(is.numeric)) %>% names()
num_cols
```



```{r}
# checking stationarity

library(tseries)

cols <- c("avg_weekly_hours_goods_producing",
          "avg_weekly_hours_manufacturing",
          "3_month_commercial_paper_minus_fedfunds",
          "3_month_treasury_c_minus_fedfunds",
          "6_month_treasury_c_minus_fedfunds",
          "1_year_treasury_c_minus_fedfunds",
          "5_year_treasury_c_minus_fedfunds",
          "10_year_treasury_c_minus_fedfunds",
          "moody_s_aaa_corporate_bond_minus_fedfunds",
          "moody_s_baa_corporate_bond_minus_fedfunds",
          "vix",
          "exports_of_gs",
          "imports_of_gs",
          "opec_crude_oil_capacity",
          "opec_crude_oil_production",
          "producer_price_index",
          "usd_index",
          "global_real_economic_activity_index")

adf_p <- function(x) {
  x <- x[is.finite(x)]
  if (length(x) < 20) return(NA_real_)
  tryCatch(adf.test(x, alternative = "stationary")$p.value, error = function(e) NA_real_)
}
kpss_p <- function(x) {
  x <- x[is.finite(x)]
  if (length(x) < 20) return(NA_real_)
  # Level stationarity (you can also try null="Trend" if you expect linear trend)
  tryCatch(kpss.test(x, null = "Level")$p.value, error = function(e) NA_real_)
}

decide_stationarity <- function(adf_pv, kpss_pv) {
  adf_reject  <- !is.na(adf_pv)  && adf_pv  < 0.05   # reject unit root → stationary
  kpss_reject <- !is.na(kpss_pv) && kpss_pv < 0.05   # reject stationarity → non-stationary
  if (adf_reject && !kpss_reject)        "stationary"
  else if (!adf_reject && kpss_reject)   "non-stationary"
  else if (adf_reject && kpss_reject)    "trend/nonstationary mix"
  else                                    "inconclusive (lean non-stationary)"
}

# Suggested transform from decision + data domain
suggest_transform <- function(x, name, decision) {
  # Domain overrides first:
  if (grepl("minus_fedfunds|_spread|term_spread", name)) {
    # Spreads are usually stationary → keep in level unless tests scream otherwise
    return(if (decision == "non-stationary") "Δ" else "none")
  }
  if (grepl("\\bvix\\b", name)) {
    return("log1p level (and optional Δlog1p)")
  }
  # Generic rule:
  if (decision == "stationary") {
    "none"
  } else {
    if (all(x[is.finite(x)] > 0, na.rm = TRUE)) "Δlog" else "Δ"
  }
}

diag_tbl <- purrr::map_dfr(cols, function(col) {
  x <- clean_data[[col]]
  ap <- adf_p(x)
  kp <- kpss_p(x)
  dec <- decide_stationarity(ap, kp)
  tibble(
    column = col,
    adf_p = ap,
    kpss_p = kp,
    decision_5pct = dec,
    suggested_transform = suggest_transform(x, col, dec)
  )
}) %>%
  arrange(match(decision_5pct, c("non-stationary",
                                 "trend/nonstationary mix",
                                 "inconclusive (lean non-stationary)",
                                 "stationary")),
          column)

print(diag_tbl, n = 100)
```




```{r}
logdiff <- function(x) log(x) - dplyr::lag(log(x), 1)
diff1   <- function(x) x - dplyr::lag(x, 1)

df_trans <- clean_data

changed_cols <- c()

for (i in seq_len(nrow(diag_tbl))) {
  col <- diag_tbl$column[i]
  how <- diag_tbl$suggested_transform[i]


  if (how == "none") next

  if (how == "Δlog") {
    
    new_name <- paste0("dlog_", col)
    changed_cols <- c(changed_cols, new_name)
    df_trans[[new_name]] <- logdiff(clean_data[[col]])
  } else if (how == "Δ") {
    
    new_name <- paste0("d_", col)
    changed_cols <- c(changed_cols, new_name)
    df_trans[[new_name]] <- diff1(clean_data[[col]])
  } else if (how == "log1p level (and optional Δlog1p)") {
    # VIX treatment
    #df_trans[[paste0("vix_log1p")]] <- log1p(clean_data[[col]])
    df_trans[[paste0("d_vix_log1p")]] <- diff1(log1p(clean_data[[col]]))
    changed_cols <- c(changed_cols, 'd_vix_log1p')
  }
}

```



```{r}
to_remove <- diag_tbl %>%
  filter(suggested_transform %in% c("Δlog", "Δ", "log1p level (and optional Δlog1p)")) %>%
  pull(column)
to_remove
```
```{r}
df_trans <- df_trans %>% select(-all_of(to_remove))
```

Now df_trans contains the same number of features as the raw data but with transformation

```{r}
changed_cols
```
```{r}

```


```{r}
#Adding more transformations after realising that the data from FRED does not help you do the transformations

df_to_transform <- read.csv('name_changed.csv')
date <- df_to_transform$date
cols_to_transform <- setdiff(names(df_to_transform), 'date')
df_to_transform <- df_to_transform[, cols_to_transform, drop = FALSE]
df_to_transform


new_transformed <- fred_transform(
  data = df_to_transform,
  type = "fred_md",
  na.rm=FALSE
)

df_final <- cbind(date = date, new_transformed)

diff_name <- c('IPB51222s','S.P.500','S.P.div.yield','S.P.PE.ratio')
additional_transformed <- fred_transform(
  data = df_to_transform[, diff_name, drop = FALSE],
  type = 'fred_md',
  codes = c(5,5,2,5),
  na.rm = FALSE)

final_transformed <- cbind(
  df_final[, setdiff(names(df_final), diff_name), drop = FALSE],
  additional_transformed
)
write.csv(final_transformed, "changingback.csv", row.names = FALSE)
df_trans <- read.csv("final_full_name.csv")





```



```{r}

#Transforming target
targets <- raw_data %>%
  transmute(
    date,
    y_lr_1m  = log(dplyr::lead(brent_spot_price, 1))  - log(brent_spot_price),
    y_lr_3m = log(dplyr::lead(brent_spot_price, 3))  - log(brent_spot_price),
    y_lr_6m  = log(dplyr::lead(brent_spot_price, 6))  - log(brent_spot_price),
    y_lr_12m = log(dplyr::lead(brent_spot_price,12))  - log(brent_spot_price)
    )


# Getting past one month lags of brent price for lag preparation
df_trans <- df_trans %>%
  mutate(r_1m = log(brent_spot_price) - lag(log(brent_spot_price), 1))
```



Create lags of up to 6
```{r}
final_df = df_trans%>% select(-brent_spot_price) %>% arrange(date)
final_df
```

```{r}
#Getting names of columns to get lagged versions of 
pred_cols <- setdiff(names(final_df), c("date"))

#Helper to build lag-k columns
lag_k <- function(df, cols, k) {
  df %>% transmute(across(all_of(cols),
                          ~ dplyr::lag(.x, k),
                          .names = "{.col}_l{k}"))
}
 

#Build lags 1 to 6 and bind
lagged_list <- map(0:6, ~ lag_k(final_df, pred_cols, .x))
X_lagged <- bind_cols(tibble(date = final_df$date), lagged_list)

final_df_1m <- X_lagged %>%
  bind_cols(targets%>% select(y_lr_1m)) %>% drop_na()

final_df_3m <- X_lagged %>%
  bind_cols(targets%>% select(y_lr_3m)) %>% drop_na()

final_df_6m <-  X_lagged %>%
  bind_cols(targets%>% select(y_lr_6m)) %>% drop_na()

final_df_12m <-  X_lagged %>%
  bind_cols(targets%>% select(y_lr_12m))%>% drop_na()
```

```{r}
#Last check to see if df are empty
sum(is.na(final_df_12m))
sum(is.na(final_df_6m))
sum(is.na(final_df_3m))
sum(is.na(final_df_1m))
```



```{r}
#Splitting data into training and testing
split_ts <- function(df, split_ratio = 0.8) {
  n <- nrow(df)
  n_train <- floor(n * split_ratio)
  train <- df[1:n_train, ]
  test  <- df[(n_train + 1):n, ]
  list(train = train, test = test)
}


splits_1m  <- split_ts(final_df_1m)
splits_3m <- split_ts(final_df_3m)
splits_6m  <- split_ts(final_df_6m)
splits_12m <- split_ts(final_df_12m)

```


```{r}


#Standardising on train set and applying it to test set

make_recipe <- function(train_df, target_col, id_cols = "date") {
  recipe(as.formula(paste(target_col, "~ .")), data = train_df) %>%
    update_role(all_of(id_cols), new_role = "id") %>%
    update_role_requirements("id", bake = TRUE) %>%   
    step_zv(all_predictors()) %>%
    step_center(all_predictors()) %>%
    step_scale(all_predictors())
}

# build one recipe per horizon
rec_1m  <- make_recipe(splits_1m$train,  target_col = "y_lr_1m")
rec_3m <- make_recipe(splits_3m$train,  target_col = "y_lr_3m")
rec_6m  <- make_recipe(splits_6m$train,  target_col = "y_lr_6m")
rec_12m <- make_recipe(splits_12m$train, target_col = "y_lr_12m")

# prep (learn means/sds on TRAIN only) and bake
prep_1m  <- prep(rec_1m,  training = splits_1m$train)
prep_3m  <- prep(rec_3m,  training = splits_3m$train)
prep_6m  <- prep(rec_6m,  training = splits_6m$train)
prep_12m <- prep(rec_12m, training = splits_12m$train)

train_1m <- bake(prep_1m,  new_data = splits_1m$train)
test_1m  <- bake(prep_1m,  new_data = splits_1m$test)
test_1m  <- dplyr::bind_cols(splits_1m$test["date"], select(test_1m,-date))

train_3m <- bake(prep_3m,  new_data = splits_3m$train)
test_3m  <- bake(prep_3m,  new_data = splits_3m$test)
test_3m  <- dplyr::bind_cols(splits_3m$test["date"], select(test_3m,-date))

train_6m <- bake(prep_6m,  new_data = splits_6m$train)
test_6m  <- bake(prep_6m,  new_data = splits_6m$test)
test_6m  <- dplyr::bind_cols(splits_6m$test["date"], select(test_6m,-date))

train_12m <- bake(prep_12m, new_data = splits_12m$train)
test_12m  <- bake(prep_12m, new_data = splits_12m$test)
test_12m  <- dplyr::bind_cols(splits_12m$test["date"], select(test_12m,-date))

```

```{r}
write.csv(train_1m, "train_1m.csv", row.names = FALSE)
write.csv(train_3m, "train_3m.csv", row.names = FALSE)
write.csv(train_6m, "train_6m.csv", row.names = FALSE)
write.csv(train_12m, "train_12m.csv", row.names = FALSE)

write.csv(test_1m, "test_1m.csv", row.names = FALSE)
write.csv(test_3m, "test_3m.csv", row.names = FALSE)
write.csv(test_6m, "test_6m.csv", row.names = FALSE)
write.csv(test_12m, "test_12m.csv", row.names = FALSE)

write.csv(final_df_1m, "final_df_1m.csv", row.names = FALSE)
write.csv(final_df_3m, "final_df_3m.csv", row.names = FALSE)
write.csv(final_df_6m, "final_df_6m.csv", row.names = FALSE)
write.csv(final_df_12m, "final_df_12m.csv", row.names = FALSE)

```






